{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN+UdVkhjVZJP8S/LD/5Ok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/botvoodoo/NLP/blob/main/nlp_raytune_hf\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xFaGTDD4gR4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install wandb\n",
        "!pip install torch --upgrade\n",
        "!pip install datasets evaluate\n",
        "!pip install xformers\n",
        "!pip install ray tune\n",
        "!pip install kaggle --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv /content/sample_data/kaggle/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "id": "kRfVYhks8RhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "metadata": {
        "id": "QRPkd5Nb8DEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ray\n",
        "from ray import tune\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset, load_metric\n",
        "from huggingface_hub import notebook_login\n",
        "import wandb\n",
        "import random\n",
        "import ray"
      ],
      "metadata": {
        "id": "2vSVY9vD4iy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "\n",
        "#Credit for helper functions: https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb\n",
        "\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import series of helper functions for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "FfcPlSDM9AEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping Data\n",
        "unzip_data(\"/content/nlp-getting-started.zip\")\n",
        "\n",
        "# Load data from the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "df = df.drop(['id', 'keyword', 'location'], axis=1) # Drop unnecessary columns\n",
        "df.rename(columns={'text': 'text', 'target': 'label'}, inplace=True) # Rename columns\n",
        "df.head()"
      ],
      "metadata": {
        "id": "620HOIKg4mSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Seed\n",
        "random_seed = (13)\n",
        "\n",
        "#Function Definition (train_model): Define a function to train a BERT model for sequence classification.\n",
        "\n",
        "#Configuration Extraction: Extract various hyperparameters and configurations like learning rate, batch size, weight decay, number of epochs, warmup steps...\n",
        "#rate scheduler type, and dropout rate from the passed config dictionary.\n",
        "\n",
        "def train_model(config, checkpoint_dir=None):\n",
        "    df = config[\"data_df\"]\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    batch_size = config[\"per_device_train_batch_size\"]\n",
        "    weight_decay = config[\"weight_decay\"]\n",
        "    num_epochs = config[\"num_train_epochs\"]\n",
        "    warmup_steps = config[\"warmup_steps\"]\n",
        "    lr_scheduler_type = config[\"lr_scheduler_type\"]\n",
        "    dropout_rate = config[\"dropout_rate\"]\n",
        "\n",
        "#Checkpoint Handling: If there's a checkpoint directory provided, load the checkpoint state to continue training from where it left off. If not, start from epoch 0.\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            checkpoint_state = torch.load(checkpoint_path)\n",
        "            start_epoch = checkpoint_state[\"epoch\"]\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "#Data Splitting and Tokenization: Split the dataset into training and validation parts and tokenize the data using BERT tokenizer.\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=random_seed)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    def tokenize_data(example):\n",
        "        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "#Data Transformation: Convert the tokenized data into datasets compatible with PyTorch, and set the format for the required columns.\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df).map(tokenize_data)\n",
        "    val_dataset = Dataset.from_pandas(val_df).map(tokenize_data)\n",
        "\n",
        "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "#Data Loading: Create DataLoaders for training and validation datasets to handle batching, shuffling, and parallel loading.\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=int(config[\"per_device_train_batch_size\"]), shuffle=True, num_workers=2)\n",
        "    valloader = DataLoader(val_dataset, batch_size=int(config[\"per_device_train_batch_size\"]), shuffle=True, num_workers=2)\n",
        "\n",
        "#Model and Loss Function Initialization: Instantiate a BERT model for sequence classification with two labels and define a cross-entropy loss function.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "#Checkpoint Model Loading (Optional): If a checkpoint exists, load the saved model state.\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
        "\n",
        "#Optimizer Initialization: Define the Adam optimizer with the given learning rate and weight decay.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "#Device Configuration: Determine the device to use (either GPU or CPU) and move the model to the chosen device.\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "#Training Loop: Perform the training loop through a specified number of epochs.\n",
        "\n",
        "#Forward pass through model. Calculation of loss.\n",
        "\n",
        "    for epoch in range(start_epoch, 10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs = data[\"input_ids\"].to(device)\n",
        "            attention_masks = data[\"attention_mask\"].to(device)\n",
        "            labels = data[\"label\"].to(device)\n",
        "\n",
        "#Update model parameters using the optimizer. Backward pass to compute gradients.\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "#Print running loss every 2000 iterations.\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "#Validation Loop: Evaluate the model on the validation set, compute validation loss and accuracy.\n",
        "\n",
        "# Validation\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(valloader, 0):\n",
        "                inputs = data[\"input_ids\"].to(device)\n",
        "                attention_masks = data[\"attention_mask\"].to(device)\n",
        "                labels = data[\"label\"].to(device)\n",
        "\n",
        "                outputs = model(inputs, attention_mask=attention_masks)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "\n",
        "                _, predicted = torch.max(outputs.logits, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "#Checkpoint Saving: Save the model and optimizer states at each epoch.\n",
        "\n",
        "# Save checkpoint\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch\n",
        "            }, checkpoint_path)\n",
        "\n",
        "# Report to Ray Tune: Report validation loss and accuracy to Ray Tune for hyperparameter tuning.\n",
        "\n",
        "        val_accuracy = correct / total * 100\n",
        "        tune.report(val_loss=val_loss / val_steps, val_accuracy=val_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Finished Training\")\n"
      ],
      "metadata": {
        "id": "CjhwLDhm4pOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "config = {\n",
        "    \"data_df\": df,\n",
        "    \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
        "    # Other hyperparameters\n",
        "}\n",
        "\n",
        "analysis = tune.run(\n",
        "    train_model,\n",
        "    config=config,\n",
        "    stop={\"val_loss\": 0.01},\n",
        "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
        "    num_samples=10\n",
        ")\n"
      ],
      "metadata": {
        "id": "AoNZSBhj4vFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
